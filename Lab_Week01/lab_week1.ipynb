{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqRSnO68lKJR"
      },
      "source": [
        "# COMP3132 - Lab Week 1\n",
        "\n",
        "# Building an LLM-Powered Chatbot: A Hands-On Guide in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILpJ2QAOWH0N"
      },
      "source": [
        "## Google Colab Configuration\n",
        "\n",
        "### Some of our labs this semester will be painfully slow if without a GPU. The easies way to get access to a GPU accelerated Jupyter notebook is to enable the `T4 GPU runtime` on Google Colab:\n",
        "\n",
        "### 1. Navigate to `Runtime`.\n",
        "### 2. Select `Change runtime type`.\n",
        "### 3. Choose `Hardware accelerator`.\n",
        "### 4. Select `T4 GPU`.\n",
        "\n",
        "### **Note:** This notebook can be run on `CPU` without any noticeable difference in performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxpgrcGYZJHG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0Gedb1EAT3Pd"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "!pip install jupyter_bokeh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVFndvYeTku2"
      },
      "source": [
        "# Online Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIizeeRWTku4"
      },
      "source": [
        "### Go to https://api.together.ai/playground/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo to chat with the model online on `togerther.ai` website and play with the chatbot by changing the configurations and hyper-parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ylsoOI0SRYO"
      },
      "source": [
        "# A Brief Theory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-cwOAjzWH0P"
      },
      "source": [
        "## Training a Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "tXq3n80DWH0P",
        "outputId": "7f830104-a61a-4571-a1bd-0eaed4eac4b7"
      },
      "outputs": [],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/PyDataGBC/PyML2026/refs/heads/main/assets/LLM_train.png'\n",
        "display(Image(image_path, width=600))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxrSLigOlTkh"
      },
      "source": [
        "## Base Vs. Chat Models\n",
        "\n",
        "### After training the LLMs with this paradigm on a very large amount of data (such as the entire internet), we will have a model, also known as a `foundation` model or `base` model, that can predict the next word repeatedly to form a sentence.\n",
        "\n",
        "### To enable the model to engage in conversations, we further fine-tune the base model using instructions, such as question-answer pairs. These models are referred to as `instruction-tuned` or `chat` models.\n",
        "\n",
        "### You can observe the different behaviors of the base and instruction-tuned models in the following slide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "CD0rO0IEWZMh",
        "outputId": "19d34572-4079-4692-f797-ef67f07c2e30"
      },
      "outputs": [],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/PyDataGBC/PyML2026/refs/heads/main/assets/baseVSinstruct.png'\n",
        "display(Image(image_path, width=800))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhQZl868Tku4"
      },
      "source": [
        "## Interacting with Model Programmatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "x1CnpZpbTku5",
        "outputId": "b734ae3c-36c0-45e5-db1d-51108d25171f"
      },
      "outputs": [],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/PyDataGBC/PyML2026/refs/heads/main/assets/modelaccess.png'\n",
        "display(Image(image_path, width=500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh1tK_PPlF9Y"
      },
      "source": [
        "# Designing Our Own Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zXImou1Tku5"
      },
      "source": [
        "## API Call to the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npNiuaiEYA2o"
      },
      "source": [
        "### Getting API KEY\n",
        "\n",
        "#### - Go to https://api.together.xyz/settings/api-keys to get your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86cIAvU0YKNY"
      },
      "source": [
        "#### Importing the API Key to Colab\n",
        "\n",
        "1. On the left-side vertical menu, select the `key` icon.\n",
        "2. Add a secret key with the following details:\n",
        "   - **Name**: `TOGETHER_API_KEY`\n",
        "   - **Value**: `<your API key>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v-Y7RV0X_NE"
      },
      "outputs": [],
      "source": [
        "# from google.colab import userdata\n",
        "# api_key = userdata.get('TOGETHER API KEY')\n",
        "api_key = 'tgp_v1_GKBdA9ptqAemxc-i2VHL1aYLG6g9ikaovTSUrxCT_A'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1J4cqDmTku6"
      },
      "source": [
        "### Function to call the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yofz4yMLN4be"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# from dotenv import load_dotenv, find_dotenv\n",
        "import warnings\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "url = \"https://api.together.xyz/inference\"\n",
        "\n",
        "headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "\n",
        "import time\n",
        "def llama(prompt,\n",
        "          add_inst=True,\n",
        "          model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "          temperature=0.0,\n",
        "          max_tokens=1024,\n",
        "          verbose=False,\n",
        "          url=url,\n",
        "          headers=headers,\n",
        "          base = 2, # number of seconds to wait\n",
        "          max_tries=3):\n",
        "\n",
        "    if add_inst:\n",
        "        prompt = f\"[INST]{prompt}[/INST]\"\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Prompt:\\n{prompt}\\n\")\n",
        "        print(f\"model: {model}\")\n",
        "\n",
        "    data = {\n",
        "            \"model\": model,\n",
        "            \"prompt\": prompt,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "\n",
        "    # Allow multiple attempts to call the API incase of downtime.\n",
        "    # Return provided response to user after 3 failed attempts.\n",
        "    wait_seconds = [base**i for i in range(max_tries)]\n",
        "\n",
        "    for num_tries in range(max_tries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data)\n",
        "            return response.json()['output']['choices'][0]['text']\n",
        "        except Exception as e:\n",
        "            if response.status_code != 500:\n",
        "                return response.json()\n",
        "\n",
        "            print(f\"error message: {e}\")\n",
        "            print(f\"response object: {response}\")\n",
        "            print(f\"num_tries {num_tries}\")\n",
        "            print(f\"Waiting {wait_seconds[num_tries]} seconds before automatically trying again.\")\n",
        "            time.sleep(wait_seconds[num_tries])\n",
        "\n",
        "    print(f\"Tried {max_tries} times to make API call to get a valid response object\")\n",
        "    print(\"Returning provided response\")\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl752Qo5Lpmd"
      },
      "source": [
        "### **Note:** Default model is `\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"` but can you can change it by finding the model name from https://api.together.ai/playground/chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0UVk9rmTku8"
      },
      "source": [
        "## General testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"prompt 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teSmKBj-Tku8"
      },
      "outputs": [],
      "source": [
        "# pass prompt to the llama function, store output as 'response' then print\n",
        "prompt = \"Tell me a funny joke about software developers.\"\n",
        "response = llama(prompt)  # temperature is a hyperparameter that controls randomness in the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtIx3E0PTku9"
      },
      "outputs": [],
      "source": [
        "prompt = \"What is the capital of France?\"\n",
        "mistral = 'mistralai/Ministral-3-14B-Instruct-2512'\n",
        "response = llama(prompt, verbose=True, add_inst=False, model=mistral)  # verbose=True will print the prompt\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response['choices'][0]['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7DSSINATku9"
      },
      "source": [
        "## Exercise 1: General testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFSl58nmTku9"
      },
      "source": [
        "#### 1. Change the `temprarature` parameter from 0.0 to 0.9 and see the difference in the responses.\n",
        "#### Note: temperature parameter is a number between 0.0 and 1.0. It controls the randomness of the responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PwICCgsTku9"
      },
      "outputs": [],
      "source": [
        "#your code here\n",
        "print(llama(\"What tags are used as controls for the chat mode?\", add_inst=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ecA__NVTku9"
      },
      "source": [
        "## Role prompting\n",
        "\n",
        "#### - Roles give context to LLMs what type of answers are desired.\n",
        "#### - LLMs often gives more consistent responses when provided with a role.\n",
        "#### - First, try standard prompt and see the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G43Y1R4HTku9"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"How can I answer this question from my friend:\n",
        "What is the meaning of life?\"\"\"\n",
        "\n",
        "response = llama(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "461pt7uDTku-"
      },
      "source": [
        "###  Now, try it by giving the model a `role`, and within the role, a `tone` using which it should respond with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBlxI0qyTku-"
      },
      "outputs": [],
      "source": [
        "role = \"\"\"Your role is a life coach \\\n",
        "who gives advice to people about living a good life.\\\n",
        "You attempt to provide unbiased advice.\n",
        "You respond in the tone of an English pirate.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "{role}\n",
        "How can I answer this question from my friend:\n",
        "What is the meaning of life?\n",
        "\"\"\"\n",
        "\n",
        "#print(prompt)\n",
        "\n",
        "response = llama(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ndaDyN-Tku-"
      },
      "source": [
        "## Excercise 2: Role prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuFXfjX2Tku-"
      },
      "source": [
        "#### Role: Beginner python tutor\n",
        "#### Task: Explain how to create a list and add an element to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZysdT1qTku-"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "prompt = \"Your are a beginner python tutor. Explain how to create a list and add an element to it.\"\n",
        "print(llama(prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limfSy2ITku-"
      },
      "source": [
        "#### Change the role to `friendly coding mentor` and see how the response changes for the same task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FpEZmfITku-"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "prompt = \"Your are a friendly coding mentor. Explain how to create a list and add an element to it.\"\n",
        "print(llama(prompt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rUfjq56Tku-"
      },
      "source": [
        "## Asking follow-up questions\n",
        "\n",
        "### Does the model have memory of the previous conversation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-XQCIVsTku-"
      },
      "outputs": [],
      "source": [
        "prompt_1 = \"What are fun activities I can do this weekend?\"\n",
        "\n",
        "response_1 = llama(prompt_1)\n",
        "print(response_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPR5leRcTku_"
      },
      "outputs": [],
      "source": [
        "prompt_2 = \"Which of these would be good for my health?\"\n",
        "response_2 = llama(prompt_2)\n",
        "print(response_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3qOB8QdTku_"
      },
      "source": [
        "#### Is the the second answer related to the first answer?\n",
        "#### **Note:** LLMs are `stateless` models, so they don't have memory of the previous conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGHE7OzBTku_"
      },
      "source": [
        "## Multi-turn prompting (chatting)\n",
        "#### In order to give the model memory of the previous conversation, you need to provide prior prompts and responses as part of the context of each new turn in the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "TXKGAF6dTku_",
        "outputId": "de32ed8b-392c-40f4-d71c-fb6304365572"
      },
      "outputs": [],
      "source": [
        "image_path = 'https://raw.githubusercontent.com/PyDataGBC/PyML2026/refs/heads/main/assets/multi_turn.png'\n",
        "display(Image(image_path, width=600))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEAyQcBJTku_"
      },
      "source": [
        "### Note: you don't need `end tag (</s>)` for the last prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qjMQfTtTku_"
      },
      "outputs": [],
      "source": [
        "chat_prompt = f\"\"\"\n",
        "<s>[INST] {prompt_1} [/INST]\n",
        "{response_1}\n",
        "</s>\n",
        "<s>[INST] {prompt_2} [/INST]\n",
        "\"\"\"\n",
        "print(chat_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnAxWREPTku_"
      },
      "source": [
        "### Note: pay attention to add_inst (add instruction) argument below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNzt7OueTkvF"
      },
      "outputs": [],
      "source": [
        "response_2 = llama(chat_prompt,\n",
        "                 add_inst=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h68xn5bnTkvF"
      },
      "outputs": [],
      "source": [
        "print(response_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1rnXleQTkvF"
      },
      "source": [
        "### Helper function to handle multi-turn prompting\n",
        "\n",
        "### **Note:** You don’t need to understand every part of the helper function. In the next section, you’ll see how to use it in your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-fpsRdygQGr"
      },
      "outputs": [],
      "source": [
        "def llama_chat(prompts,\n",
        "               responses,\n",
        "               model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "               temperature=0.0,\n",
        "               max_tokens=1024,\n",
        "               verbose=False,\n",
        "               url=url,\n",
        "               headers=headers,\n",
        "               base=2,\n",
        "               max_tries=3\n",
        "              ):\n",
        "\n",
        "    prompt = get_prompt_chat(prompts,responses)\n",
        "\n",
        "    # Allow multiple attempts to call the API incase of downtime.\n",
        "    # Return provided response to user after 3 failed attempts.\n",
        "    wait_seconds = [base**i for i in range(max_tries)]\n",
        "\n",
        "    for num_tries in range(max_tries):\n",
        "        try:\n",
        "            response = llama(prompt=prompt,\n",
        "                             add_inst=False,\n",
        "                             model=model,\n",
        "                             temperature=temperature,\n",
        "                             max_tokens=max_tokens,\n",
        "                             verbose=verbose,\n",
        "                             url=url,\n",
        "                             headers=headers\n",
        "                            )\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            if response.status_code != 500:\n",
        "                return response.json()\n",
        "\n",
        "            print(f\"error message: {e}\")\n",
        "            print(f\"response object: {response}\")\n",
        "            print(f\"num_tries {num_tries}\")\n",
        "            print(f\"Waiting {wait_seconds[num_tries]} seconds before automatically trying again.\")\n",
        "            time.sleep(wait_seconds[num_tries])\n",
        "\n",
        "    print(f\"Tried {max_tries} times to make API call to get a valid response object\")\n",
        "    print(\"Returning provided response\")\n",
        "    return response\n",
        "\n",
        "\n",
        "def get_prompt_chat(prompts, responses):\n",
        "  prompt_chat = f\"<s>[INST] {prompts[0]} [/INST]\"\n",
        "  for n, response in enumerate(responses):\n",
        "    prompt = prompts[n + 1]\n",
        "    prompt_chat += f\"\\n{response}\\n </s><s>[INST] \\n{ prompt }\\n [/INST]\"\n",
        "\n",
        "  return prompt_chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC0gE4EETkvG"
      },
      "source": [
        "### How to use the helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laPRgntYTkvG"
      },
      "outputs": [],
      "source": [
        "prompt_1 = \"What are fun activities I can do this weekend?\"\n",
        "response_1 = llama(prompt_1)\n",
        "print(response_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v0kHVQz_gqg"
      },
      "outputs": [],
      "source": [
        "print(response_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqZGqej2TkvG"
      },
      "outputs": [],
      "source": [
        "prompt_2 = \"Which of these would be good for my health?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owh02eUjTkvG"
      },
      "outputs": [],
      "source": [
        "prompts = [prompt_1,prompt_2]\n",
        "responses = [response_1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3g-pXDOTkvG"
      },
      "outputs": [],
      "source": [
        "# Pass prompts and responses to llama_chat function\n",
        "response_2 = llama_chat(prompts,responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRyRAi7dTkvG"
      },
      "outputs": [],
      "source": [
        "print(response_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZF11qERTkvG"
      },
      "source": [
        "### Excercise 3: Multi-turn prompting\n",
        "\n",
        "### Ask this follow-up question: \"Which of these activites would be fun with friends?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB56VJpPTkvG"
      },
      "outputs": [],
      "source": [
        "prompt_3 = \"Which of these activites would be fun with friends?\"\n",
        "\n",
        "#your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMXa5DP3Dj1B",
        "outputId": "77f45840-c5b0-48f8-e945-f3c2f250c640"
      },
      "outputs": [],
      "source": [
        "prompts = [prompt_1,prompt_2, prompt_3]\n",
        "responses = [response_1, response_2]\n",
        "\n",
        "# Pass prompts and responses to llama_chat function\n",
        "response_3 = llama_chat(prompts,responses)\n",
        "\n",
        "print(response_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqvSPYnTkvG"
      },
      "source": [
        "### OrderBot\n",
        "\n",
        "#### We can `automate the collection of user prompts and model responses` to build a  OrderBot.\n",
        "\n",
        "#### The OrderBot will take orders at a pizza restaurant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgVECoD8WH0U"
      },
      "outputs": [],
      "source": [
        "# Define the bot's role and menu\n",
        "role = \"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "You first greet the customer, then start collecting the order, \\\n",
        "and then asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "The menu includes \\\n",
        "\n",
        "Primary Category: Pizza\n",
        "  Secondary Category: \\\n",
        "    pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "    cheese pizza   10.95, 9.25, 6.50 \\\n",
        "    eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "Primary Category: Sides\n",
        "  Secondary Category: \\\n",
        "    fries 4.50, 3.50 \\\n",
        "    greek salad 7.25 \\\n",
        "Primary Category: Toppings: \\\n",
        "  Secondary Category: \\\n",
        "    extra cheese 2.00, \\\n",
        "    mushrooms 1.50 \\\n",
        "    sausage 3.00 \\\n",
        "    canadian bacon 3.50 \\\n",
        "    AI sauce 1.50 \\\n",
        "    peppers 1.00 \\\n",
        "Primary Category: Drinks \\\n",
        "  Secondary Category: \\\n",
        "    coke 3.00, 2.00, 1.00 \\\n",
        "    sprite 3.00, 2.00, 1.00 \\\n",
        "    bottled water 5.00 \\\n",
        "\n",
        "the price based on size example:\n",
        "pepperoni pizza  large = 12.95,\n",
        "pepperoni pizza  medium = 10.00,\n",
        "pepperoni pizza  small = 7.00 \\\n",
        "\n",
        "For all items also check the size with the customer first\n",
        "Do not forget to ask for drinks and sides.\n",
        "Do not add any items extra by yourself\n",
        "\"\"\"\n",
        " # accumulate messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqa-1yg7WH0U"
      },
      "source": [
        "## Excercise 4: Orderbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbvuUb_yWH0U"
      },
      "outputs": [],
      "source": [
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "prompts.append(role)\n",
        "responses.append('Hi, what would you like to order today?')\n",
        "\n",
        "print(prompts)\n",
        "print(responses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(responses[0])\n",
        "while True:\n",
        "    p = input(\"User: \")\n",
        "    if p.lower() in ['done', 'exit', 'quit', 'bye']:\n",
        "        print(\"Chatbot: Thank you for your order! Have a great day!\")\n",
        "        break\n",
        "\n",
        "    prompts.append(p)\n",
        "    response = llama_chat(prompts, responses)\n",
        "    responses.append(response)\n",
        "    print(f\"\\nChatbot: {response}\\n\")\n",
        "    # terminate the loop if the user types 'done', 'exit', 'quit', or bye\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z_SMMmmWH0U"
      },
      "source": [
        "### Printing the order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7P-7fJsTkvH"
      },
      "outputs": [],
      "source": [
        "role = 'create a json summary of the food order. Itemize the price for each item\\\n",
        " The fields should be 1) pizza, include type of pizza and size and price 2) list of toppings with price 3) list of drinks, include size and price\\\n",
        "          4) list of sides include size and price 5)total price - just include items in my order and do not add anything by yourself'\n",
        "\n",
        "messages = prompts.copy()\n",
        "messages.append(role)\n",
        "\n",
        "response = llama_chat(messages, responses)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYUYSe0eTkvH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": ".venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
